{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os  \n",
    "import ast #to convert the string into dict\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.read_csv('./Articles/newsarticles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['body_no_html'] = df_articles['body'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows without user needs (265)\n",
    "df_articles.dropna(subset=['userNeeds'],inplace = True)\n",
    "df_articles.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dictionary is reversed --> now we have principal/secondaire as keys\n",
    "dict_column = []\n",
    "for i in range(0,df_articles.shape[0]):\n",
    "    inv_map = dict(zip(ast.literal_eval(df_articles['userNeeds'][i]).values(), ast.literal_eval(df_articles['userNeeds'][i]).keys()))\n",
    "    dict_column.append(inv_map)\n",
    "\n",
    "df_articles['userNeeds_dict'] = dict_column\n",
    "userNeed = []\n",
    "for i in range(0,df_articles.shape[0]):\n",
    "    userNeed.append(df_articles['userNeeds_dict'][i].get('principal'))\n",
    "\n",
    "df_articles['userNeeds_clean'] = userNeed\n",
    "df_articles.drop(['userNeeds_dict'],axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.to_csv('Articles_withCleanedUN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rts_0 = dd.read_csv('/www.rts.ch_accesslogs.log',sep = ' ', header = None,)\n",
    "df_rts = df_rts_0.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Html number\n",
      "2833789\n"
     ]
    }
   ],
   "source": [
    "df_rts = df_rts.drop([0,1,3,5,6,7,8,10,11,12,13,14,16,17,18],axis = 1)\n",
    "df_rts.columns = ['Timestamp','User_IP','Url','User_Agent']\n",
    "index_html = df_rts.query('Url.str.contains(\".html$\", na= False)').index\n",
    "print('Html number')\n",
    "print(len(index_html))\n",
    "df_rts_html = df_rts[df_rts.index.isin(index_html)]\n",
    "df_rts_html.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'info/culture/cinema/13891031-drii-winter-cascadeuses-et-la-ligne-recompenses-aux-prix-du-cinema-suisse.html'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rts_html.loc[65,'Url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {'culture': r'culture',\n",
    "          'economie': r'economie',\n",
    "          'suisse': r'suisse',\n",
    "          'monde': r'monde',\n",
    "          'sciences-tech': r'sciences-tech',\n",
    "          'sport': r'sport',\n",
    "          'environnement' : r'environnement'}\n",
    "\n",
    "def extract_topic_and_id(url):\n",
    "    if pd.isna(url):\n",
    "        return 'other', None\n",
    "    for topic, pattern in topics.items():\n",
    "        if re.search(pattern, url):\n",
    "            escenic_id_match = re.search(r'/(\\d+)-', url)\n",
    "            escenic_id = escenic_id_match.group(1) if escenic_id_match else None\n",
    "            return topic, escenic_id\n",
    "    return 'other', None  # If no topic matches, return 'other' and None for Escenic ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to extract the topic and Escenic ID\n",
    "df_rts_html[['Topic', 'Escenic_ID']] = df_rts_html['Url'].apply(lambda x: pd.Series(extract_topic_and_id(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: culture\n",
      "Escenic ID: 13891031\n"
     ]
    }
   ],
   "source": [
    "topic, escenic_id = extract_topic_and_id(df_rts_html.loc[65,'Url'])\n",
    "\n",
    "print(\"Topic:\", topic)\n",
    "print(\"Escenic ID:\", escenic_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
